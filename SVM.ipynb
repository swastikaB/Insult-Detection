{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine using Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1503109689.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to sbhat1@scu.edu and will expire on August 12, 2018.\n"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "data = gl.SFrame.read_csv(\"/Users/swastika.b/Documents/Swastika/SantaClaraEdu/courses/Machine Learning/Project/FinalCode/training_data.csv\",verbose= False)\n",
    "test_data = gl.SFrame.read_csv(\"/Users/swastika.b/Documents/Swastika/SantaClaraEdu/courses/Machine Learning/Project/FinalCode/train.csv\",verbose= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    \n",
    "    import re\n",
    "    import string\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    \n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    lower_case_text = text.translate(None, string.punctuation).lower()\n",
    "    words_list = word_tokenize(lower_case_text)\n",
    "    text2 = []\n",
    "    for word in words_list:\n",
    "        text0 = word.decode('ascii', 'ignore')\n",
    "        text1 = regex.sub(u'', text0)\n",
    "        if not text1 == u'':\n",
    "            if not text1 in stopwords.words('english'):\n",
    "                text2.append(porter.stem(text1))\n",
    "    return text2\n",
    "    \n",
    "\n",
    "data['comment_clean'] = data['Comment'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_text(words):\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['comment_string'] = data['comment_clean'].apply(create_text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    lower_case_text = text.translate(None, string.punctuation).lower()\n",
    "    return lower_case_text\n",
    "    \n",
    "#data['comment_string'] = data['Comment'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def split_into_lemmas(comments):\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 8), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    analyze = bigram_vectorizer.build_analyzer()\n",
    "    return analyze(comments)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=split_into_lemmas,stop_words='english',strip_accents='ascii').fit(data['comment_string'])\n",
    "text_transformed = vectorizer.transform(data['comment_string'])\n",
    "tfidf_transformer = TfidfTransformer().fit(text_transformed)\n",
    "tfidf_transformed_text = tfidf_transformer.transform(text_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#classifier_nb = LinearSVC(random_state=0, penalty=\"l1\", dual=False).fit(tfidf_transformed_text, data['Insult'])\n",
    "classifier_nb = LinearSVC(random_state=0).fit(tfidf_transformed_text, data['Insult'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K - Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swastika.b/Documents/Softwares/anaconda/envs/gl-env/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7051454138702461"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "y = (data['Insult']).to_numpy()\n",
    "predicted = cross_val_predict(classifier_nb, tfidf_transformed_text, y, cv=10)\n",
    "metrics.accuracy_score(y, predicted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['comment_clean'] = test_data['Comment'].apply(transform_text)\n",
    "test_data['comment_string'] = test_data['comment_clean'].apply(create_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text_transformed = vectorizer.transform(test_data['comment_string'])\n",
    "tfidf_transformed_test_text = tfidf_transformer.transform(test_text_transformed)\n",
    "predicted_class = classifier_nb.predict(tfidf_transformed_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69014441347859135"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "true_class = np.array(test_data['Insult'])\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_class, predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Insult</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Comment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">comment_string</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"You with the 'racist'<br>screen name\\n\\nYou are a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">racist screen name<br>pieceofshit ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[1 rows x 3 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tInsult\tint\n",
       "\tComment\tstr\n",
       "\tcomment_string\tstr\n",
       "\n",
       "Rows: 1\n",
       "\n",
       "Data:\n",
       "+--------+-------------------------------+--------------------------------+\n",
       "| Insult |            Comment            |         comment_string         |\n",
       "+--------+-------------------------------+--------------------------------+\n",
       "|   1    | \"You with the 'racist' scr... | racist screen name pieceofshit |\n",
       "+--------+-------------------------------+--------------------------------+\n",
       "[1 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data = test_data[16:17]\n",
    "sample_test_data.select_columns(['Insult','Comment','comment_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"You with the 'racist' screen name\n",
      "\n",
      "You are a PieceOfShit..........\"']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype: str\n",
       "Rows: 1\n",
       "['racist screen name pieceofshit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print sample_test_data['Comment']\n",
    "sample_test_data['comment_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_test_data = [\"You’re a moron, truth is beyond your reach\", \"I’ll take that temp...I really hate the heat\"]\n",
    "tokenized_sample_data = []\n",
    "for text in sample_test_data:\n",
    "    tokenized_sample_data.append(transform_text(text))\n",
    "string_sample_data = []\n",
    "for text in tokenized_sample_data:\n",
    "    string_sample_data.append(create_text(text))\n",
    "sample_text_transformed = vectorizer.transform(string_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample_text_transformed = vectorizer.transform(sample_test_data['comment_string'])\n",
    "tfidf_transformed_sample_text = tfidf_transformer.transform(sample_text_transformed)\n",
    "predicted_class = classifier_nb.predict(tfidf_transformed_sample_text)\n",
    "classifier_nb.predict(tfidf_transformed_sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
